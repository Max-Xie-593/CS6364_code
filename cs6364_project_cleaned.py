# -*- coding: utf-8 -*-
"""CS6364_Project_Cleaned.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Oxl7Jj9Pk_i0QZB4tRnCvgVddZIlDxj
"""

!pip install nptyping
!pip install thop

# !7z x fashion-mnist.7z
# !7z x mnist.7z

# Commented out IPython magic to ensure Python compatibility.
# torch
import torch
import torch.nn       as     nn
import torch.optim    as     optim
from   torch.autograd import Function

# torch utils
import torchvision
import torchvision.transforms as transforms
import torchvision.datasets as datasets

# additional libraries
from typing import Any, Iterable, MutableSequence, Tuple, Text, Tuple, List, Mapping, Sequence, Union, Dict
import os
import urllib.request
import zipfile
import time
import math
import numpy             as np
import matplotlib.pyplot as plt
import pandas as pd
from thop import profile, clever_format
from tqdm.notebook import tqdm
from nptyping import NDArray
import sklearn
from sklearn.metrics import classification_report, confusion_matrix
from itertools import product
# %matplotlib inline

hyper_parameters: Mapping[
    str,
    Sequence[Union[int, float]]
] = {
    "n_linear_warmup_epochs": [1, 3, 5],
    "learning_rate_max": [0.01, 0.001, 0.0001],
}

"""# Functions"""

def create_numpy_data(file: Text) -> Tuple[NDArray,NDArray]:
    cvs_data = pd.read_csv(file)
    return (
        np.reshape(
            np.array(
                cvs_data.iloc[:,1:],
                dtype=np.int32
            ),
            (
                len(cvs_data.index),1,28,28
            )
        ),
        np.array(
            cvs_data.iloc[:,0],
            dtype=np.int32
        )
    )

def create_torch_dataloader(
    training_data_0: NDArray, training_label_0: NDArray,
    validation_data_0: NDArray, validation_label_0: NDArray,
    test_data_0: NDArray, test_label_0: NDArray,
    
    training_data_1: NDArray, training_label_1: NDArray,
    validation_data_1: NDArray, validation_label_1: NDArray,
    test_data_1: NDArray, test_label_1: NDArray,

    batch_size: int = 32
) -> Tuple[
           torch.utils.data.DataLoader,
           torch.utils.data.DataLoader,
           torch.utils.data.DataLoader
    ]:
    

    X_train = torch.from_numpy(
        np.concatenate(
            (training_data_0,training_data_1)
        )
    ).float()
    X_validation = torch.from_numpy(
        np.concatenate(
            (validation_data_0,validation_data_1)
        )
    ).float()
    X_test = torch.from_numpy(
        np.concatenate(
            (test_data_0,test_data_1)
        )
    ).float()

    # concept - 0
    modified_labels_training_0 = np.zeros(len(training_label_0))
    modified_labels_validation_0 = np.zeros(len(validation_label_0))
    modified_labels_testing_0 = np.zeros(len(test_label_0))
    # tangible - 1
    modified_labels_training_1 = np.ones(len(training_label_1))
    modified_labels_validation_1 = np.ones(len(validation_label_1))
    modified_labels_testing_1 = np.ones(len(test_label_1))

    y_train = torch.from_numpy(
        np.concatenate(
            (modified_labels_training_0,modified_labels_training_1)
        )
    ).long()
    y_validation = torch.from_numpy(
        np.concatenate(
            (modified_labels_validation_0,modified_labels_validation_1)
        )
    ).long()
    y_test = torch.from_numpy(
        np.concatenate(
            (modified_labels_testing_0,modified_labels_testing_1)
        )
    ).long()

    print('training data shape: {shape}'.format(shape=X_train.shape))
    print('validation data shape: {shape}'.format(shape=X_validation.shape))
    print('test data shape: {shape}'.format(shape=X_test.shape))
    print('training labels shape: {shape}'.format(shape=y_train.shape))
    print('validation labels shape: {shape}'.format(shape=y_validation.shape))
    print('test labels shape: {shape}'.format(shape=y_test.shape))

    transform_train = transforms.Compose(
        [
         transforms.RandomCrop((20, 20)),
         transforms.Pad(4),
         transforms.Normalize((0.5),(0.5))
        ]
    )
    transform_validation_test = transforms.Compose(
        [
         transforms.CenterCrop((24, 24)),
         transforms.Pad(2),
         transforms.Normalize((0.5),(0.5))
        ]
    )

    return (
        torch.utils.data.DataLoader(
            torch.utils.data.TensorDataset(
                transform_train(X_train),y_train
            ),
            batch_size=batch_size,
            shuffle=True,
            num_workers=4,
            drop_last=True
        ),
        torch.utils.data.DataLoader(
            torch.utils.data.TensorDataset(
                transform_validation_test(X_validation),y_validation
            ),
            batch_size=batch_size,
            shuffle=False,
            num_workers=4,
            drop_last=True
        ),
        torch.utils.data.DataLoader(
            torch.utils.data.TensorDataset(
                transform_validation_test(X_test),y_test
            ),
            batch_size=batch_size,
            shuffle=False,
            num_workers=4,
            drop_last=True
        )
    )

def train(
    model: nn.Module,
    dataloader_train: torch.utils.data.DataLoader,
    dataloader_val: torch.utils.data.DataLoader,
    file_name: Text,
    file_save: int,
    file_load: int,
    params: Dict[str,Union[int,float]],
    max_epochs: int
) -> List[Tuple[float,float,float]]:
    assert params['n_linear_warmup_epochs'] < max_epochs, 'initial epochs should be less than max epochs'

    # training (linear warm up with cosine decay learning rate)
    TRAINING_LR_MAX = params['learning_rate_max']
    TRAINING_LR_INIT_SCALE   = 0.01
    TRAINING_LR_INIT_EPOCHS  = params['n_linear_warmup_epochs']
    TRAINING_LR_FINAL_SCALE  = 0.01
    TRAINING_LR_FINAL_EPOCHS = max_epochs - TRAINING_LR_INIT_EPOCHS
    TRAINING_NUM_EPOCHS      = TRAINING_LR_INIT_EPOCHS + TRAINING_LR_FINAL_EPOCHS
    TRAINING_LR_INIT         = TRAINING_LR_MAX*TRAINING_LR_INIT_SCALE
    TRAINING_LR_FINAL        = TRAINING_LR_MAX*TRAINING_LR_FINAL_SCALE
    # start epoch
    start_epoch = 0
    
    # learning rate schedule
    def lr_schedule(epoch):
        # linear warmup followed by cosine decay
        if epoch < TRAINING_LR_INIT_EPOCHS:
            lr = (TRAINING_LR_MAX - TRAINING_LR_INIT)*(float(epoch)/TRAINING_LR_INIT_EPOCHS) + TRAINING_LR_INIT
        else:
            lr = (TRAINING_LR_MAX - TRAINING_LR_FINAL)*max(0.0, math.cos(((float(epoch) - TRAINING_LR_INIT_EPOCHS)/(TRAINING_LR_FINAL_EPOCHS - 1.0))*(math.pi/2.0))) + TRAINING_LR_FINAL
        return lr

    # error (softmax cross entropy)
    criterion = nn.CrossEntropyLoss()

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)

    # specify the device as the GPU if present with fallback to the CPU
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # print(device)

    # transfer the network to the device
    model.to(device)

    # model loading
    if file_load == 1:
        checkpoint = torch.load(file_name)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1

    # cycle through the epochs
    training_statistics: List[Tuple[float,float]] = []
    for epoch in range(start_epoch, TRAINING_NUM_EPOCHS):

        # initialize train set statistics
        model.train()
        training_loss = 0.0
        num_batches   = 0

        # set the learning rate for the epoch
        for g in optimizer.param_groups:
            g['lr'] = lr_schedule(epoch)

        # cycle through the train set
        for data in tqdm(dataloader_train):

            # extract a batch of data and move it to the appropriate device
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            # zero the parameter gradients
            optimizer.zero_grad()

            # forward pass, loss, backward pass and weight update
            outputs = model(inputs)
            loss    = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # update statistics
            training_loss = training_loss + loss.item()
            num_batches   = num_batches + 1

        # initialize test set statistics
        model.eval()
        test_correct = 0
        test_total   = 0

        val_loss = 0.0
        val_num_batches   = 0
        # no weight update / no gradient needed
        with torch.no_grad():

            # cycle through the test set
            for data in tqdm(dataloader_val):

                # extract a batch of data and move it to the appropriate device
                inputs, labels = data
                inputs, labels = inputs.to(device), labels.to(device)

                # forward pass and prediction
                outputs      = model(inputs)
                predicted = torch.max(outputs.data, 1)[1]

                # loss    = criterion(outputs, labels)
                # loss.backward()
                # optimizer.step()

                # update test set statistics
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
                val_loss += criterion(outputs, labels).item()
                val_num_batches += 1


        # epoch statistics
        print('Epoch {0:2d} lr = {1:8.6f} avg loss = {2:8.6f} avg validation loss = {3:8.6f} accuracy = {4:5.2f}'.format(
            epoch,
            lr_schedule(epoch),
            (training_loss/num_batches)/32,
            (val_loss/val_num_batches)/32,
            (100.0*test_correct/test_total)
            )
        )
        # print(f"Epoch {epoch} lr = {lr_schedule(epoch)} avg train loss = {np.float32((training_loss/num_batches)/32)}  avg val loss = {np.float32(val_loss/val_num_batches/32)}")
        training_statistics.append(
            (
                (training_loss/num_batches)/32,
                (val_loss/val_num_batches)/32,
                (100.0*test_correct/test_total)
            )
        )

    # model saving
    # to use this for checkpointing put this code block inside the training loop at the end (e.g., just indent it 4 spaces)
    # and set 'epoch' to the current epoch instead of TRAINING_NUM_EPOCHS - 1; then if there's a crash it will be possible
    # to load this checkpoint and restart training from the last complete epoch instead of having to start training at the
    # beginning
        if file_save == 1:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict()
            }, file_name)

    return training_statistics

def hyper_train(
    model: nn.Module,
    dataloader_train: torch.utils.data.DataLoader,
    dataloader_val: torch.utils.data.DataLoader,
    file_name: Text,
    file_save: int,
    file_load: int,
    max_epochs: int
) -> List[
          Tuple[
                Dict[
                     str,
                     Union[
                           int,
                           float
                          ]
                    ],
                List[
                     Tuple[
                           float,
                           float,
                           float
                    ]
                ]
        ]
    ]:
    hyper_param_training_stats = []
    for params in product(*map(hyper_parameters.get,list(hyper_parameters))):
        params_dict = dict(zip(list(hyper_parameters),params))
        print(params_dict)
        training_stats = train(
            model,
            dataloader_train,
            dataloader_val,
            file_name,
            file_save,
            file_load,
            params_dict,
            max_epochs
        )
        hyper_param_training_stats.append((params_dict,training_stats))
    
    print(np.array(hyper_param_training_stats))
    return (hyper_param_training_stats)

def output_MACS_Params(model: nn.Module) -> None:
    size = torch.randn(1,1,28,28)
    macs, params = profile(model, inputs = (size,),verbose=False)
    print('---------------------MACs & Params of Model---------------------')
    print(macs,params)
    macs, params = clever_format([macs,params],"%.3f")
    print(macs,params)

def output_results(model: nn.Module, dataloader_test: torch.utils.data.DataLoader, file_name: Text) -> None:
    # checkpoint = torch.load(file_name)
    # model.load_state_dict(checkpoint['model_state_dict'])

    torch.cuda.empty_cache()
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # print(device)

    model.to(device)

    y_pred = []
    y_true = []
    with torch.no_grad():

        # cycle through the test set
        for data in dataloader_test:

            # extract a batch of data and move it to the appropriate device
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            # forward pass and prediction
            outputs      = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            y_pred.append(predicted.cpu().numpy())
            y_true.append(labels.cpu().numpy())

    flatten_y_pred = np.ndarray.flatten(np.array([a.squeeze().tolist() for a in y_pred]))
    flatten_y_true = np.ndarray.flatten(np.array([a.squeeze().tolist() for a in y_true]))

    print('---------------------classification report---------------------')
    print(classification_report(flatten_y_true,flatten_y_pred))
    print('---------------------confusion matrix---------------------')
    print(confusion_matrix(flatten_y_true,flatten_y_pred))

"""# Pre-Processing"""

mnist_data_train, mnist_labels_train = create_numpy_data('./mnist_train.csv')
mnist_data_val, mnist_labels_val = create_numpy_data('./mnist_val.csv')
mnist_data_test, mnist_labels_test = create_numpy_data('./mnist_test.csv')
fashion_data_train, fashion_labels_train = create_numpy_data('./fashion-mnist_train.csv')
fashion_data_val, fashion_labels_val = create_numpy_data('./fashion-mnist_val.csv')
fashion_data_test, fashion_labels_test = create_numpy_data('./fashion-mnist_test.csv')

(dataloader_train, dataloader_val, dataloader_test) = create_torch_dataloader(
        mnist_data_train, mnist_labels_train,
        mnist_data_val, mnist_labels_val,
        mnist_data_test, mnist_labels_test,
        fashion_data_train, fashion_labels_train,
        fashion_data_val, fashion_labels_val,
        fashion_data_test, fashion_labels_test
)

"""# ResNetV2"""

# ResNetV2 bottleneck
class ResNetV2Bottleneck(nn.Module):

    # initialization
    def __init__(self, in_channels, residual_channels, out_channels, stride_num):

        # parent initialization
        super(ResNetV2Bottleneck, self).__init__()

        # identity
        if ((in_channels != out_channels) or (stride_num > 1)):
            self.conv0_present = True
            self.conv0         = nn.Conv2d(in_channels, out_channels, (1, 1), stride=(stride_num, stride_num), padding=(0, 0), dilation=(1, 1), groups=1, bias=False, padding_mode='zeros')
        else:
            self.conv0_present = False

        # residual
        self.block_1 = nn.ModuleList()
        self.block_1.append(nn.BatchNorm2d(in_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))
        self.block_1.append(nn.ReLU())
        self.block_1.append(nn.Conv2d(in_channels, residual_channels, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False, padding_mode='zeros'))
        
        self.block_2 = nn.ModuleList()
        self.block_2.append(nn.BatchNorm2d(residual_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))
        self.block_2.append(nn.ReLU())
        self.block_2.append(nn.Conv2d(residual_channels, residual_channels, (3, 3), stride=(stride_num, stride_num), padding=(1, 1), dilation=(1, 1), groups=1, bias=False, padding_mode='zeros'))

        self.block_3 = nn.ModuleList()
        self.block_3.append(nn.BatchNorm2d(residual_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))
        self.block_3.append(nn.ReLU())
        self.block_3.append(nn.Conv2d(residual_channels, out_channels, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False, padding_mode='zeros'))

    # forward path
    def forward(self, x):

        res = x
        # residual
        for layer in self.block_1:
            res = layer(res)

        for layer in self.block_2:
            res = layer(res)

        for layer in self.block_3:
            res = layer(res)

        # identity
        if (self.conv0_present == True):
            x = self.conv0(x)

        # summation
        x = x + res

        # return
        return x

# ResNetV2 model
class ResNetModel(nn.Module):
    # initialization
    def __init__(self):

        # parent initialization
        super(ResNetModel, self).__init__()

        # encoder tail
        self.enc_tail = nn.ModuleList()
        self.enc_tail.append(nn.Conv2d(1, 32, (3, 3), stride=(1, 1), padding=(3, 3), dilation=(1, 1), groups=1, bias=False, padding_mode='zeros'))


        # encoder level 0
        self.enc_0 = nn.ModuleList()
        self.enc_0.append(ResNetV2Bottleneck(32, 1 * 32, 128, 1))
        for n in range(4 - 1):
            self.enc_0.append(ResNetV2Bottleneck(128, 1 * 32, 128, 1))

        # encoder level 1
        self.enc_1 = nn.ModuleList()
        self.enc_1.append(ResNetV2Bottleneck(128,64, 256, 2))
        for n in range(6 - 1):
            self.enc_1.append(ResNetV2Bottleneck(256,64, 256, 1))

        # encoder level 2
        self.enc_2 = nn.ModuleList()
        self.enc_2.append(ResNetV2Bottleneck(256, 128, 512, 2))
        for n in range(3 - 1):
            self.enc_2.append(ResNetV2Bottleneck(512, 128, 512, 1))

        # encoder level 3
        self.enc_3 = nn.ModuleList()
        self.enc_3.append(nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))
        self.enc_3.append(nn.ReLU())

        # decoder
        self.dec = nn.ModuleList()
        self.dec.append(nn.AdaptiveAvgPool2d((1, 1)))
        self.dec.append(nn.Flatten())
        self.dec.append(nn.Linear(512,2,bias=True))
        # data_num_classes = 2 for concept and intangable
        

    # forward path
    def forward(self, x):

        # encoder tail
        for layer in self.enc_tail:
            x = layer(x)
        
        # encoder level 0
        for layer in self.enc_0:
            x = layer(x)

        # encoder level 1
        for layer in self.enc_1:
            x = layer(x)

        # encoder level 2
        for layer in self.enc_2:
            x = layer(x)

        # encoder level 3 
        for layer in self.enc_3:
            x = layer(x)

        # decoder
        for layer in self.dec:
            x = layer(x)

        # return
        return x

resnet_model = ResNetModel()
print(resnet_model)

output_MACS_Params(resnet_model)

resnet_hyper_stats = hyper_train(
    resnet_model,
    dataloader_train,
    dataloader_val,
    'resnet.pt',
    1,
    0,
    10
)

print(np.array(resnet_hyper_stats))

train(resnet_model,dataloader_train,dataloader_val,'resnetBest.pt',1,0,{'learning_rate_max':0.001,'n_linear_warmup_epochs':1},10)

output_results(resnet_model,dataloader_test,'resnetBest.pt')

"""# MobileNetV2"""

def make_divisible(dividend: float, divisor: int = 8) -> int:
    # It was used in Google's implementation so we decided to copy it over
    return int(np.ceil(dividend * 1.0 / divisor) * divisor)


class MobileNetV2ModelBottleneck(nn.Module):
    def __init__(
        self,
        input_channels: int,
        output_channels: int,
        stride: int,
        expansion_multiplier: int,
    ):
        super(MobileNetV2ModelBottleneck, self).__init__()

        self.use_residual_connection: bool = (
            stride == 1 and input_channels == output_channels
        )

        expansion_channels: int = int(round(input_channels * expansion_multiplier))

        convolution_layers: MutableSequence[nn.Module] = (
            [
                # 1x1 Kernel Projection-Wise Linear Convolution.
                # Note: We do this to make sure that before we do depth-wise
                # convolution the input channel size is the same as the
                # expansion channel size
                nn.Conv2d(input_channels, expansion_channels, 1, 1, 0, bias=False),
                nn.BatchNorm2d(expansion_channels),
                nn.ReLU6(inplace=True),
            ]
            if expansion_multiplier != 1
            else []
        )
        convolution_layers.extend(
            [
                # Depth-Wise Convolution
                nn.Conv2d(
                    expansion_channels,
                    expansion_channels,
                    3,
                    stride,
                    1,
                    groups=expansion_channels,
                    bias=False,
                ),
                nn.BatchNorm2d(expansion_channels),
                nn.ReLU6(inplace=True),
                # 1x1 Kernel Projection-Wise Linear Convolution
                nn.Conv2d(expansion_channels, output_channels, 1, 1, 0, bias=False),
                nn.BatchNorm2d(output_channels),
            ]
        )

        self.convolution_layers = nn.Sequential(*convolution_layers)

    def forward(self, x: Any) -> Any:
        if self.use_residual_connection:
            return x + self.convolution_layers(x)
        else:
            return self.convolution_layers(x)


class MobileNetV2Model(nn.Module):
    def __init__(self, num_classes: int = 2, width_mult: float = 1.0):
        super(MobileNetV2Model, self).__init__()

        # Setup Input (Tail Section). The first layer should be a 3x3
        # convolution into batch norm into an ReLU6 activation function
        input_channel: int = 32
        layers: MutableSequence[nn.Module] = [
            nn.Sequential(
                nn.Conv2d(1, input_channel, 3, 2, 1, bias=False),
                nn.BatchNorm2d(input_channel),
                nn.ReLU6(inplace=True),
            )
        ]

        # Build Inverted Residual Bottleneck Layers (Body Section)
        body_hyperparameters: Iterable[Tuple[int, int, int, int]] = [
            (1, 16, 1, 1),
            (6, 24, 2, 2),
            (6, 32, 3, 2),
            (6, 64, 4, 2),
            (6, 96, 3, 1),
            (6, 160, 3, 2),
            (6, 320, 1, 1),
        ]

        for (
            expansion_multiplier,
            output_channels,
            num_repeats,
            stride,
        ) in body_hyperparameters:
            if expansion_multiplier > 1:
                output_channels = make_divisible(output_channels * width_mult)
            for i in range(1, num_repeats):
                layers.append(
                    MobileNetV2ModelBottleneck(
                        input_channel,
                        output_channels,
                        stride if i == 0 else 1,
                        expansion_multiplier,
                    )
                )
                input_channel = output_channels

        # Add last layer in body section. Combine Tail and Body sections together
        last_layer_channels: int = (
            make_divisible(1280 * width_mult) if width_mult > 1.0 else 1280
        )
        self.features = nn.Sequential(
            *layers,
            nn.Sequential(
                nn.Conv2d(input_channel, last_layer_channels, 1, 1, 0, bias=False),
                nn.BatchNorm2d(last_layer_channels),
                nn.ReLU6(inplace=True),
            ),
        )

        # Build Head Section (classifier)
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Dropout(0.2),
            nn.Linear(last_layer_channels,num_classes)
        )

        # Initilize Weights of the Layers
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x: Any) -> Any:
        x = self.features(x)
        return self.classifier(x)


mobilenet_v2_model = MobileNetV2Model()
print(mobilenet_v2_model)

output_MACS_Params(mobilenet_v2_model)

mobilenet_v2_hyper_stats = hyper_train(
    mobilenet_v2_model,
    dataloader_train,
    dataloader_val,
    'mobilenetv2.pt',
    1,
    0,
    10
)

print(np.array(mobilenet_v2_hyper_stats))

train(mobilenet_v2_model,dataloader_train,dataloader_val,'mobilenet_v2_Best.pt',1,0,{'learning_rate_max':0.01,'n_linear_warmup_epochs':1},10)

output_results(mobilenet_v2_model,dataloader_test,'mobilenet_v2_Best.pt')

train(mobilenet_v2_model,dataloader_train,dataloader_val,'mobilenet_v2_Best_2.pt',1,0,{'learning_rate_max':0.001,'n_linear_warmup_epochs':1},10)

output_results(mobilenet_v2_model,dataloader_test,'mobilenet_v2_Best_2.pt')

"""# MobileNetV3 (Small)"""

class HSigmoid(nn.Module):
    def __init__(self, inplace: bool = True):
        super(HSigmoid, self).__init__()
        self.relu = nn.ReLU6(inplace=inplace)

    def forward(self, x: float) -> float:
        return self.relu(x + 3) / 6


class HSwish(nn.Module):
    def __init__(self, inplace: bool = True):
        super(HSwish, self).__init__()
        self.sigmoid = HSigmoid(inplace=inplace)

    def forward(self, x: float) -> float:
        return x * self.sigmoid(x)


class SELayer(nn.Module):
    def __init__(self, channel: int, reduction: int = 4):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fully_connected_layer = nn.Sequential(
            nn.Linear(channel, make_divisible(channel // reduction)),
            nn.ReLU(inplace=True),
            nn.Linear(make_divisible(channel // reduction), channel),
            HSigmoid(),
        )

    def forward(self, x: Any) -> Any:
        b, c = x.size()[:2]
        y = self.avg_pool(x).view(b, c)
        y = self.fully_connected_layer(y).view(b, c, 1, 1)
        return x * y


class InvertedResidualBottleneck(nn.Module):
    def __init__(
        self,
        input_channels: int,
        expansion_channels: int,
        oup: int,
        kernel_size: int,
        stride: int,
        use_se: bool,
        activation_function_name: str,
    ):
        super(InvertedResidualBottleneck, self).__init__()

        self.use_residual_connection: bool = stride == 1 and input_channels == oup

        squeeze_excite_layer: nn.Module = (
            SELayer(expansion_channels) if use_se else nn.Identity()
        )

        activation_function: nn.Module = (
            nn.ReLU(inplace=True) if activation_function_name == "ReLU" else HSwish()
        )

        convolution_layers: MutableSequence[nn.Module] = (
            [
                # 1x1 Kernel Projection-Wise Linear Convolution.
                # Note: We do this to make sure that before we do depth-wise
                # convolution the input channel size is the same as the
                # expansion channel size
                nn.Conv2d(input_channels, expansion_channels, 1, 1, 0, bias=False),
                nn.BatchNorm2d(expansion_channels),
                activation_function,
            ]
            if input_channels != expansion_channels
            else []
        )
        convolution_layers.extend(
            [
                # Depth-Wise Convolution
                nn.Conv2d(
                    expansion_channels,
                    expansion_channels,
                    kernel_size,
                    stride,
                    (kernel_size - 1) // 2,
                    groups=expansion_channels,
                    bias=False,
                ),
                nn.BatchNorm2d(expansion_channels),
                # Squeeze-Excite Layer
                squeeze_excite_layer,
                activation_function,
                # 1x1 Kernel Projection-Wise Linear Convolution
                nn.Conv2d(expansion_channels, oup, 1, 1, 0, bias=False),
                nn.BatchNorm2d(oup),
            ]
        )

        self.convolution_layers = nn.Sequential(*convolution_layers)


    def forward(self, x: Any) -> Any:
        if self.use_residual_connection:
            return x + self.convolution_layers(x)
        else:
            return self.convolution_layers(x)


class MobileNetV3Small(nn.Module):
    def __init__(self, num_classes: int = 2, width_mult: float = 1.0):
        super(MobileNetV3Small, self).__init__()

        # Setup Input (Tail Section). The first layer should be a 3x3
        # convolution into batch norm into an H-swish activation function
        input_channel: int = make_divisible(1 * width_mult)
        # input_channel: int = 16
        layers: MutableSequence[nn.Module] = [
            nn.Sequential(
                nn.Conv2d(1, input_channel, 3, 2, 1, bias=False),
                nn.BatchNorm2d(input_channel),
                HSwish(),
            )
        ]

        # Build Inverted Residual Bottleneck Layers (Body Section)
        body_hyperparameters: Iterable[Tuple[int, int, int, bool, str, int]] = [
            (3, 16, 16, True, "ReLU", 2),
            (3, 72, 24, False, "ReLU", 2),
            (3, 88, 24, False, "ReLU", 1),
            (5, 96, 40, True, "H-Swish", 2),
            (5, 240, 40, True, "H-Swish", 1),
            (5, 240, 40, True, "H-Swish", 1),
            (5, 120, 48, True, "H-Swish", 1),
            (5, 144, 48, True, "H-Swish", 1),
            (5, 288, 96, True, "H-Swish", 2),
            (5, 576, 96, True, "H-Swish", 1),
            (5, 576, 96, True, "H-Swish", 1),
        ]

        for (
            kernel_size,
            expansion_size,
            output_channels,
            use_squeeze_excitation,
            activation_function,
            stride,
        ) in body_hyperparameters:
            output_channels = make_divisible(output_channels * width_mult)
            expansion_channels = make_divisible(expansion_size * width_mult)
            layers.append(
                InvertedResidualBottleneck(
                    input_channel,
                    expansion_channels,
                    output_channels,
                    kernel_size,
                    stride,
                    use_squeeze_excitation,
                    activation_function,
                )
            )
            input_channel = output_channels

        # Combine Tail and Body sections together
        self.features = nn.Sequential(*layers)

        # Build Head Section
        head_output_size: int = make_divisible(960 * width_mult)
        self.conv = nn.Sequential(
            nn.Conv2d(input_channel, head_output_size, 1, 1, 0, bias=False),
            nn.BatchNorm2d(head_output_size),
            HSwish(),
        )
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        output_channel: int = (
            make_divisible(1024 * width_mult) if width_mult > 1.0 else 1024
        )
        self.classifier = nn.Sequential(
            nn.Linear(head_output_size, output_channel),
            HSwish(),
            nn.Dropout(0.2),
            nn.Linear(output_channel, num_classes),
        )


        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x: Any) -> Any:
        x = self.features(x)
        x = self.conv(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

mobilenetV3_model = MobileNetV3Small()
print(mobilenetV3_model)

output_MACS_Params(mobilenetV3_model)

mobilenetV3_hyper_stats = hyper_train(
    mobilenetV3_model,
    dataloader_train,
    dataloader_val,
    'mobilenetV3.pt',
    1,
    0,
    10
)

print(np.array(mobilenetV3_hyper_stats))

train(mobilenetV3_model,dataloader_train,dataloader_val,'mobilenet_v3_Best.pt',1,0,{'learning_rate_max':0.0001,'n_linear_warmup_epochs':1},10)

output_results(mobilenetV3_model,dataloader_test,'mobilenet_v3_Best.pt')

"""# Results"""

output_results(resnet_model,dataloader_test,'resnet.pt')

output_results(mobilenet_v2_model,dataloader_test,'mobilenetv2.pt')

output_results(mobilenetV3_model,dataloader_test,'mobilenetV3.pt')