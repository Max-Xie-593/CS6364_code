# -*- coding: utf-8 -*-
"""HW8_MaxXie_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lxldi2CqgiJH3NLE4C8PrtvcppwXB-2x
"""

!pip install nptyping

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from tqdm.notebook import tqdm
from nptyping import NDArray
from typing import Tuple, List, Text

DISCOUNT = 0.7
MOVE_REWARD = -5
START_PROBABILITY = 0.04
NUM_EPOCHS = 1000

class GameBoard(object):

    def __init__(self):
        self.table: NDArray  = np.array([
            [1,0,0,0,0],
            [0,0,0,0,0],
            [0,0,0,0,0],
            [0,0,0,0,0],
            [0,0,0,0,1]
        ])
        self.X, self.Y = self.table.shape
        self.terminal = {
            tuple(x) for x in np.argwhere(self.table == 1)
        }
        self.possible_moves = {
            0: lambda x,y: (x + 1, y) if x < self.X - 1 else (x,y), # down
            1: lambda x,y: (x - 1, y) if x > 0 else (x,y), # up
            2: lambda x,y: (x, y - 1) if y > 0 else (x,y), # left
            3: lambda x,y: (x, y + 1) if y < self.Y - 1 else (x,y) # right
        }

    def move(self, x: int, y: int, action: int) -> Tuple[int,int]:
        try:
            return self.possible_moves[action](x,y)
        except KeyError as e:
            raise Exception("You are Boosted")

class Learner(object):

    def __init__(self, discount: float = DISCOUNT, reward: int = MOVE_REWARD):
        self.board = GameBoard()
        self.discount: float = discount
        self.reward: int = reward
        self.q_values = self._initialize_q_values()

    def __str__(self):
        return '\n'.join((action + '\n' + str(self.q_values[i])) for i, action in enumerate(['down','up','left','right']))

    @property
    def q_values(self) -> NDArray:
        return self._q_values

    @q_values.setter
    def q_values(self, values: NDArray) -> None:
        self._q_values = values


    def _initialize_q_values(self) -> NDArray:
        values = np.full((len(self.board.possible_moves), self.board.X, self.board.Y), self.reward - 1, dtype=float)
        for action in self.board.possible_moves:
            for terminalX, terminalY in self.board.terminal:
                newX, newY = self.board.move(terminalX,terminalY,action)
                if (terminalX,terminalY) != (newX,newY):
                    values[action,terminalX,terminalY] = np.NINF
                    values[action,newX,newY] = self.reward
                else:
                    values[action,terminalX,terminalY] = np.inf
        return values

    def learn(self) -> None:
        values = self.q_values
        while True:
            change: bool = False
            for action in self.board.possible_moves:
                for X, Y in map(tuple,np.array(np.meshgrid(range(self.board.X),range(self.board.Y))).T.reshape(-1,2)):
                    if (X,Y) in self.board.terminal:
                        continue
                    newX, newY = self.board.move(X,Y,action)
                    if (newX, newY) in self.board.terminal:
                        continue
                    q_location_values = values[action,X,Y]
                    values[action,X,Y] = self.reward + self.discount * np.max([self.q_values[:,newX,newY]])
                    if q_location_values != values[action,X,Y]:
                        change = True
            if not change:
                break
            self.q_values = values

class DeepQ(nn.Module):

    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Linear(2,32),
            nn.ReLU(),
            nn.Linear(32,8),
            nn.ReLU(),
            nn.Linear(8,16),
            nn.ReLU(),
            nn.Linear(16,4),
        ])
    
    def forward(self,x):
        for layer in self.layers:
            x = layer(x)
        return x

def output_table_values(values: NDArray):
    print("Max Q-Values at each state:",np.max(values,axis=0),sep='\n',end='\n\n')
    actions = str(np.argmax(values,axis=0))
    for i, action in enumerate(['↓','↑','←','→']):
        actions = actions.replace(str(i),action)
    print("Recommended action at each state (ignore terminal states):",actions,sep='\n',end='\n\n')

def output_state_moves(values: NDArray) -> None:

    def sequence(values: NDArray, x: int, y: int) -> Text:

        def move(x: int, y: int, action: int) -> Tuple[int,int]:
            possible_moves = {
                0: lambda x,y: (x + 1, y) if x < 4 else (x,y), # down
                1: lambda x,y: (x - 1, y) if x > 0 else (x,y), # up
                2: lambda x,y: (x, y - 1) if y > 0 else (x,y), # left
                3: lambda x,y: (x, y + 1) if y < 4 else (x,y) # right
            }
            try:
                return possible_moves[action](x,y)
            except KeyError as e:
                raise Exception("You are Boosted")

        terminals = {
            (0,0), (4,4)
        }
        
        x_hold, y_hold = x,y
        sequence: List[Text] = []
        while (x_hold,y_hold) not in terminals:
            sequence.append(str(values[x_hold,y_hold]))
            x_hold, y_hold = move(x_hold,y_hold,values[x_hold,y_hold])
        action = ['↓','↑','←','→']
        sequence = [x.replace(x,action[int(x)]) for x in sequence]
        return "sequence of moves at " + "({0:d},{1:d}): ".format(x,y) + ", ".join(sequence)

    actions = np.argmax(values,axis=0)
    print(sequence(actions,3,3))
    print(sequence(actions,4,2))
    print(sequence(actions,2,4))

"""# QUESTION 1"""

def q1():
    gamer: Learner = Learner()
    print("Initial Q-Table:",gamer,sep='\n',end='\n\n')
    gamer.learn()
    print("Final Q-Table:",gamer,sep='\n',end='\n\n')
    output_table_values(gamer.q_values)
    output_state_moves(gamer.q_values)

q1()

"""# QUESTION 2"""

board = GameBoard()
model = DeepQ()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)

def output_q2():
    q_table = np.zeros((len(board.possible_moves),board.X,board.Y))
    print("Initial Q-Table:",q_table,sep='\n',end='\n\n')
    for (X, Y) in map(tuple,np.array(np.meshgrid(range(board.X),range(board.Y))).T.reshape(-1,2)):
        with torch.no_grad():
            input = torch.tensor([X,Y]).float()
            input = input.to(device)
            output = model(input)
            q_table[:,X,Y] = output.cpu().numpy()
    
    print("Final Q-Table:",q_table,sep='\n',end='\n\n')
    output_table_values(q_table)
    output_state_moves(q_table)

def q2():
    for epoch in range(NUM_EPOCHS):
        training_loss = 0.0
        for (X, Y) in map(tuple,np.array(np.meshgrid(range(board.X),range(board.Y))).T.reshape(-1,2)):
            input1 = torch.tensor([X,Y]).float()
            input1 = input1.to(device)
            predict = model(input1)
            estimate = np.zeros(4)
            for action in board.possible_moves:
                if (X,Y) in board.terminal:
                    continue
                (newX, newY) = board.move(X,Y,action)
                with torch.no_grad():
                    input2 = torch.tensor([newX,newY]).float()
                    input2 = input2.to(device)
                    output = model(input2)
                    estimate[action] = MOVE_REWARD + DISCOUNT * max(output).item()
            estimate = torch.from_numpy(estimate).float()
            estimate = estimate.to(device)
            loss = criterion(predict, estimate)
            training_loss += loss.item()
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        print('Epoch {0:2d} avg loss = {1:8.6f}'.format(epoch, training_loss/25))

q2()

output_q2()